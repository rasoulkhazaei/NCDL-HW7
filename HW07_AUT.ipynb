{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSvBQoszjYJS"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/bin/python3.11' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "!pip install timm\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvXzfQqgi6lT",
        "papermill": {
          "duration": 3.862689,
          "end_time": "2021-04-05T08:01:49.835804",
          "exception": false,
          "start_time": "2021-04-05T08:01:45.973115",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from tqdm.autonotebook import tqdm\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZW25G0vr4kw"
      },
      "source": [
        "### Download dataset\n",
        "You should have a kaggle account"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2y6l5An9jIxI"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle --upgrade\n",
        "os.environ['KAGGLE_USERNAME'] = \"XXXXXX\"\n",
        "os.environ['KAGGLE_KEY'] = \"XXXXXXXXXXXXXXXXXX\"\n",
        "\n",
        "!kaggle datasets download -d adityajn105/flickr8k\n",
        "!unzip flickr8k.zip\n",
        "dataset = \"8k\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF8XzKlSi6lT"
      },
      "source": [
        "## Some pre-preocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieBwPS2Qi6lT",
        "papermill": {
          "duration": 1.449907,
          "end_time": "2021-04-05T08:01:51.298515",
          "exception": false,
          "start_time": "2021-04-05T08:01:49.848608",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "if dataset == \"8k\":\n",
        "  df = pd.read_csv(\"captions.txt\")\n",
        "  df['id'] = [id_ for id_ in range(df.shape[0] // 5) for _ in range(5)]\n",
        "  df.to_csv(\"captions.csv\", index=False)\n",
        "  df = pd.read_csv(\"captions.csv\")\n",
        "  image_path = \"/content/Images\"\n",
        "  captions_path = \"/content\"\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o54bL4gji6lU",
        "papermill": {
          "duration": 0.012732,
          "end_time": "2021-04-05T08:01:51.324144",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.311412",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ulGHg9ai6lV",
        "papermill": {
          "duration": 0.377383,
          "end_time": "2021-04-05T08:01:51.714313",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.336930",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "    debug = False\n",
        "    image_path = image_path\n",
        "    captions_path = captions_path\n",
        "    batch_size = 32\n",
        "    num_workers = 2\n",
        "    head_lr = 1e-3\n",
        "    image_encoder_lr = 1e-4\n",
        "    text_encoder_lr = 1e-5\n",
        "    weight_decay = 1e-3\n",
        "    patience = 1\n",
        "    factor = 0.8\n",
        "    epochs = 4\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model_name = 'resnet50'\n",
        "    image_embedding = 2048\n",
        "    text_encoder_model = \"distilbert-base-uncased\"\n",
        "    text_embedding = 768\n",
        "    text_tokenizer = \"distilbert-base-uncased\"\n",
        "    max_length = 200\n",
        "\n",
        "    pretrained = True # for both image encoder and text encoder\n",
        "    trainable = True # for both image encoder and text encoder\n",
        "    temperature = 1.0\n",
        "\n",
        "    # image size\n",
        "    size = 224\n",
        "\n",
        "    # for projection head; used for both image and text encoders\n",
        "    projection_dim = 128\n",
        "    dropout = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8OQpI9Xi6lV",
        "papermill": {
          "duration": 0.012746,
          "end_time": "2021-04-05T08:01:51.740070",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.727324",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_piLd4dxi6lV",
        "papermill": {
          "duration": 0.023459,
          "end_time": "2021-04-05T08:01:51.776328",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.752869",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class AvgMeter:\n",
        "    def __init__(self, name=\"Metric\"):\n",
        "        self.name = name\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.avg, self.sum, self.count = [0] * 3\n",
        "\n",
        "    def update(self, val, count=1):\n",
        "        self.count += count\n",
        "        self.sum += val * count\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __repr__(self):\n",
        "        text = f\"{self.name}: {self.avg:.4f}\"\n",
        "        return text\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group[\"lr\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM65_Loji6lW",
        "papermill": {
          "duration": 0.012817,
          "end_time": "2021-04-05T08:01:51.802043",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.789226",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c71Td1ELvF-E"
      },
      "source": [
        "The dataset returns images and encoded captions as a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9V91XcNi6lW",
        "papermill": {
          "duration": 0.025532,
          "end_time": "2021-04-05T08:01:51.840523",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.814991",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class CLIPDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
        "        \"\"\"\n",
        "        image_filenames and cpations must have the same length; so, if there are\n",
        "        multiple captions for each image, the image_filenames must have repetitive\n",
        "        file names\n",
        "        \"\"\"\n",
        "\n",
        "        self.image_filenames = image_filenames\n",
        "        self.captions = list(captions)\n",
        "        self.encoded_captions = tokenizer(\n",
        "            list(captions), padding=True, truncation=True, max_length=CFG.max_length\n",
        "        )\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            key: torch.tensor(values[idx])\n",
        "            for key, values in self.encoded_captions.items()\n",
        "        }\n",
        "\n",
        "        image = cv2.imread(f\"{CFG.image_path}/{self.image_filenames[idx]}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = self.transforms(image=image)['image']\n",
        "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
        "        item['caption'] = self.captions[idx]\n",
        "\n",
        "        return item\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.captions)\n",
        "\n",
        "\n",
        "\n",
        "def get_transforms(mode=\"train\"):\n",
        "    if mode == \"train\":\n",
        "        return A.Compose(\n",
        "            [\n",
        "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
        "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "            ]\n",
        "        )\n",
        "    else:\n",
        "        return A.Compose(\n",
        "            [\n",
        "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
        "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "            ]\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk394aMmi6lX",
        "papermill": {
          "duration": 0.012853,
          "end_time": "2021-04-05T08:01:51.866433",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.853580",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Image Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Lnwx026i6lY"
      },
      "source": [
        "We use the timm library, which comprises numerous pre-trained image models. It returns a tensor of size 2048 for each image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0flfMTRi6lY",
        "papermill": {
          "duration": 0.027706,
          "end_time": "2021-04-05T08:01:51.907283",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.879577",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encode images to a fixed size vector\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(\n",
        "            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
        "        )\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKRoQ0o9i6lY"
      },
      "source": [
        "## Text Encoder (TO DO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6jTZWtoi6lZ"
      },
      "source": [
        "We utilize the DistilBertModel from HuggingFace Transformers, which generates a 756-size vector for each input token (word). Similar to the original Bert model, the CLS token embedding is use to represent the entire caption."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "am5VR4Ezi6lZ",
        "papermill": {
          "duration": 0.027706,
          "end_time": "2021-04-05T08:01:51.907283",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.879577",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
        "        super().__init__()\n",
        "        if pretrained:\n",
        "            self.model = DistilBertModel.from_pretrained(model_name)\n",
        "        else:\n",
        "            self.model = DistilBertModel(config=DistilBertConfig())\n",
        "\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "        # we are using the CLS token hidden representation as the sentence's embedding\n",
        "        self.target_token_idx = 0\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = output.last_hidden_state # This is an tensor of size (BATCH_SIZE, num_words, 756)\n",
        "        ### TO DO: Just one Line ###\n",
        "        ### You shoud return the embeding of the CLS token. CLS token index is self.target_token_idx\n",
        "        return ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDKLE4cKi6lZ"
      },
      "source": [
        "## Projection Head (TO DO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h64obouni6lZ"
      },
      "source": [
        "After encoding both images and texts into fixed-size vectors of 2048 and 768 dimensions respectively, we need to project them onto a new space. This new world space to have similar dimensions for both image and text vectors so that we can compare them. By doing so, we can segregate non-relevant image and text vectors and pull together those that match. To achieve this, we use a code that projects the 2048 and 768 dimensional vectors into a 128 dimensional space, where we can easily compare them.\n",
        "\n",
        "Two instances of this class will be created to project image and text vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJbY9Yrui6la",
        "papermill": {
          "duration": 0.027706,
          "end_time": "2021-04-05T08:01:51.907283",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.879577",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim, # the size of the input vector (2048 for images and 768 for texts)\n",
        "        projection_dim=CFG.projection_dim, #  the the size of the output vector (Use 128)\n",
        "        dropout=CFG.dropout\n",
        "    ):\n",
        "        ### TO DO: Whole Class ###\n",
        "        ### It should be an MLP that project embeding_dim to projection_dim. Dont forget to use activation functions,layer norms and dropouts!\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(self, x):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUs23eeoi6la",
        "papermill": {
          "duration": 0.012961,
          "end_time": "2021-04-05T08:01:51.933336",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.920375",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## CLIP: Putting it all to gather (TO DO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGdEUF830mb1"
      },
      "source": [
        "This class encompasses all the logic behind CLIP. It projects images and text into a 128-dimensional space and computes the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MQnmwsWi6lc",
        "papermill": {
          "duration": 0.025366,
          "end_time": "2021-04-05T08:01:51.972338",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.946972",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class CLIPModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        temperature=CFG.temperature,\n",
        "        image_embedding=CFG.image_embedding,\n",
        "        text_embedding=CFG.text_embedding,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.text_encoder = TextEncoder()\n",
        "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
        "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # Getting Image and Text Features\n",
        "        image_features = self.image_encoder(batch[\"image\"])\n",
        "        text_features = self.text_encoder(\n",
        "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "        )\n",
        "        ### TO DO ###\n",
        "        ### Project both features into 128-dimentional space using above defined ProjectionHeads\n",
        "        image_embeddings = ...\n",
        "        text_embeddings = ...\n",
        "\n",
        "        ### TO DO ###\n",
        "        ### Calculate loss and return it. You can set the temperture parameter to 1. (See figure 2 of the homework)\n",
        "        ...\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "def cross_entropy(preds, targets, reduction='none'):\n",
        "    log_softmax = nn.LogSoftmax(dim=-1)\n",
        "    loss = (-targets * log_softmax(preds)).sum(1)\n",
        "    if reduction == \"none\":\n",
        "        return loss\n",
        "    elif reduction == \"mean\":\n",
        "        return loss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXkzSurfi6ld",
        "papermill": {
          "duration": 0.013097,
          "end_time": "2021-04-05T08:01:51.998635",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.985538",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYrHf4yQi6ld"
      },
      "source": [
        "Here are some funtions to help us load train and valid dataloaders, our model and then train and evaluate our model on those. There's not much going on here; just simple training loop and utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUSC32Cei6ld",
        "papermill": {
          "duration": 0.041512,
          "end_time": "2021-04-05T08:01:52.054352",
          "exception": false,
          "start_time": "2021-04-05T08:01:52.012840",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def make_train_valid_dfs():\n",
        "    dataframe = pd.read_csv(f\"{CFG.captions_path}/captions.csv\")\n",
        "    max_id = dataframe[\"id\"].max() + 1 if not CFG.debug else 100\n",
        "    image_ids = np.arange(0, max_id)\n",
        "    np.random.seed(42)\n",
        "    valid_ids = np.random.choice(\n",
        "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
        "    )\n",
        "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
        "    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
        "    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
        "    return train_dataframe, valid_dataframe\n",
        "\n",
        "\n",
        "def build_loaders(dataframe, tokenizer, mode):\n",
        "    transforms = get_transforms(mode=mode)\n",
        "    dataset = CLIPDataset(\n",
        "        dataframe[\"image\"].values,\n",
        "        dataframe[\"caption\"].values,\n",
        "        tokenizer=tokenizer,\n",
        "        transforms=transforms,\n",
        "    )\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=CFG.batch_size,\n",
        "        num_workers=CFG.num_workers,\n",
        "        shuffle=True if mode == \"train\" else False,\n",
        "    )\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4FcULUYi6le"
      },
      "source": [
        "Here's a handy function to train our model. There's not much happening here; just loading the batches, feeding them to the model and stepping the optimizer and lr_scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jkjp0BEPi6le",
        "papermill": {
          "duration": 0.041512,
          "end_time": "2021-04-05T08:01:52.054352",
          "exception": false,
          "start_time": "2021-04-05T08:01:52.012840",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n",
        "    loss_meter = AvgMeter()\n",
        "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
        "    for batch in tqdm_object:\n",
        "        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
        "        loss = model(batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if step == \"batch\":\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        count = batch[\"image\"].size(0)\n",
        "        loss_meter.update(loss.item(), count)\n",
        "\n",
        "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
        "    return loss_meter\n",
        "\n",
        "\n",
        "def valid_epoch(model, valid_loader):\n",
        "    loss_meter = AvgMeter()\n",
        "\n",
        "    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
        "    for batch in tqdm_object:\n",
        "        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
        "        loss = model(batch)\n",
        "\n",
        "        count = batch[\"image\"].size(0)\n",
        "        loss_meter.update(loss.item(), count)\n",
        "\n",
        "        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n",
        "    return loss_meter\n",
        "\n",
        "\n",
        "def main():\n",
        "    train_df, valid_df = make_train_valid_dfs()\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "    train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n",
        "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
        "\n",
        "\n",
        "    model = CLIPModel().to(CFG.device)\n",
        "    params = [\n",
        "        {\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr},\n",
        "        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr},\n",
        "        {\"params\": itertools.chain(\n",
        "            model.image_projection.parameters(), model.text_projection.parameters()\n",
        "        ), \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay}\n",
        "    ]\n",
        "    optimizer = torch.optim.AdamW(params, weight_decay=0.)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode=\"min\", patience=CFG.patience, factor=CFG.factor\n",
        "    )\n",
        "    step = \"epoch\"\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    for epoch in range(CFG.epochs):\n",
        "        print(f\"Epoch: {epoch + 1}\")\n",
        "        model.train()\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            valid_loss = valid_epoch(model, valid_loader)\n",
        "\n",
        "        if valid_loss.avg < best_loss:\n",
        "            best_loss = valid_loss.avg\n",
        "            torch.save(model.state_dict(), \"best.pt\")\n",
        "            print(\"Saved Best Model!\")\n",
        "\n",
        "        lr_scheduler.step(valid_loss.avg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vBQ2Wuyi6le"
      },
      "source": [
        "Running the next cell start training the model. Put the kernel on GPU mode. Every epoch should take about 8 minutes on GPU if you are using 8k version (even one epoch is enough!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G3S841Vi6le",
        "papermill": {
          "duration": 16449.265031,
          "end_time": "2021-04-05T12:36:01.333760",
          "exception": false,
          "start_time": "2021-04-05T08:01:52.068729",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiMjVEVci6le"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-dteKJwi6lf"
      },
      "source": [
        "### Getting Image Embeddings (TO DO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JTMqjOwi6lf"
      },
      "source": [
        "In this function, we are loading the model that we saved after training, feeding it images in validation set and returning the image_embeddings with shape (valid_set_size, 256) and the model itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKkDZNP3i6lf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_image_embeddings(valid_df, model_path):\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
        "\n",
        "    model = CLIPModel().to(CFG.device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=CFG.device))\n",
        "    model.eval()\n",
        "\n",
        "    valid_image_embeddings = []\n",
        "    ### TO DO ###\n",
        "    ### Loop trought validation dataloader and get 128-dim embeding of all images\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(valid_loader):\n",
        "            ...\n",
        "    return model, torch.cat(valid_image_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySr-oGpMi6lf"
      },
      "outputs": [],
      "source": [
        "_, valid_df = make_train_valid_dfs()\n",
        "model, image_embeddings = get_image_embeddings(valid_df, \"best.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP81I_wxi6lf"
      },
      "source": [
        "### Finding Matches (TO DO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LF3NabVi6lg"
      },
      "source": [
        "This function does the final task that we wished our model would be capable of: it gets the model, image_embeddings, and a text query. It will display the most relevant images from the validation set! Isn't it amazing? Let's see how it performs after all!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "un6RzF4Ri6lg",
        "papermill": {
          "duration": 0.025647,
          "end_time": "2021-04-05T12:36:01.385717",
          "exception": false,
          "start_time": "2021-04-05T12:36:01.360070",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def find_matches(model, image_embeddings, query, image_filenames, n=9):\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "    encoded_query = tokenizer([query])\n",
        "    batch = {\n",
        "        key: torch.tensor(values).to(CFG.device)\n",
        "        for key, values in encoded_query.items()\n",
        "    }\n",
        "    with torch.no_grad():\n",
        "        text_features = model.text_encoder(\n",
        "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "        )\n",
        "        text_embeddings = model.text_projection(text_features)\n",
        "\n",
        "    ### TO DO ###\n",
        "    ### Find and plot n nearest image to given query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoOu3JE8i6lg"
      },
      "source": [
        "This is how we use this function. Aaaannnndddd the results:\n",
        "(The results in the blog post and the one at the beginning of the notebook were achieved with training on the 30k version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waXVzbioi6lg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "find_matches(model,\n",
        "             image_embeddings,\n",
        "             query=\"This is a dog\",\n",
        "             image_filenames=valid_df['image'].values,\n",
        "             n=9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMHjv42x3ise"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
